{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NLP Model with sarcasm dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step 0. Library Import\n",
    "Step 1. Load Dataset\n",
    "Step 2. Data Preprocessing\n",
    "     2-a. Train/Test split\n",
    "     2-b. Tokenizer\n",
    "     2-c. Pad Sequences\n",
    "     2-d. label type : list -> numpy array\n",
    "Step 3. Modeling\n",
    "Step 4. Model Compile\n",
    "Step 5. Model Checkpoint\n",
    "Step 6. Model Fit\n",
    "Step 7. Model Evaluate & Save\n",
    "Step 8. Reload Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "url_save_path = './data/sarcasm.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/sarcasm.json', <http.client.HTTPMessage at 0x7fbdac1f8f10>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(url, url_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url_save_path) as f:\n",
    "    json_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
       "  'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
       "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
       "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
       "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
       "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       "  'is_sarcastic': 0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-a. Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for data in json_dataset:\n",
    "    X.append(data['headline'])\n",
    "    y.append(data['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline :  former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "is sarcastic ? : No\n",
      "headline :  the 'roseanne' revival catches up to our thorny political mood, for better and worse\n",
      "is sarcastic ? : Yes\n",
      "headline :  mom starting to fear son's web series closest thing she will have to grandchild\n",
      "is sarcastic ? : Yes\n",
      "headline :  boehner just wants wife to listen, not come up with alternative debt-reduction ideas\n",
      "is sarcastic ? : Yes\n",
      "headline :  j.k. rowling wishes snape happy birthday in the most magical way\n",
      "is sarcastic ? : Yes\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('headline : ', X[i])\n",
    "    print('is sarcastic ? :', ['No' if i == 0 else 'Yes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_valid = X[train_size:]\n",
    "y_valid = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-b. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[328, 1, 799, 1, 1, 47, 389, 1, 1, 6, 1, 1],\n",
       " [4, 1, 1, 1, 23, 2, 161, 1, 390, 1, 6, 251, 9, 889],\n",
       " [153, 890, 2, 891, 1, 1, 595, 1, 221, 133, 36, 45, 2, 1],\n",
       " [1, 38, 213, 382, 2, 1, 29, 288, 23, 10, 1, 1, 1, 958],\n",
       " [715, 672, 1, 1, 1, 662, 553, 5, 4, 92, 1, 90]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 30, 1, 1, 5, 519, 109],\n",
       " [202, 1, 8, 31, 1, 1, 2, 854, 773],\n",
       " [18, 380, 191, 2, 915, 76, 8, 4, 1],\n",
       " [1, 1, 299, 337, 3, 1, 1],\n",
       " [162, 1, 1, 6, 1, 1, 348, 1]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-c. Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "pad_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train, maxlen=max_length, truncating=trunc_type, padding=pad_type)\n",
    "X_valid_padded = pad_sequences(X_valid, maxlen=max_length, truncating=trunc_type, padding=pad_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[328,   1, 799,   1,   1,  47, 389,   1,   1,   6,   1,   1,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,   1,   1,  30,   1,   1,   5, 519, 109,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_padded[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 120) (6709, 120)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_padded.shape, X_valid_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-d. label type : list -> numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Datatype : \n",
      "<class 'list'> <class 'list'>\n",
      "X_padded Datatype : \n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "-------------------------\n",
      "y Datatype : \n",
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print('X Datatype : ')\n",
    "print(type(X_train), type(X_valid))\n",
    "print('X_padded Datatype : ')\n",
    "print(type(X_train_padded), type(X_valid_padded))\n",
    "print('-------------------------')\n",
    "print('y Datatype : ')\n",
    "print(type(y_train), type(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final X Datatype : \n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "-------------------------\n",
      "final y Datatype : \n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('final X Datatype : ')\n",
    "print(type(X_train_padded), type(X_valid_padded))\n",
    "print('-------------------------')\n",
    "print('final y Datatype : ')\n",
    "print(type(y_train), type(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "# vocab_size = 1000\n",
    "# max_length = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, dropout=0.5)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           16000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 120, 128)          41472     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 120, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 259,777\n",
      "Trainable params: 259,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'my_checkpoint.ckpt'\n",
    "cp = ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4571 - acc: 0.7689\n",
      "Epoch 00001: val_loss improved from inf to 0.39125, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.4571 - acc: 0.7689 - val_loss: 0.3912 - val_acc: 0.8198\n",
      "Epoch 2/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3567 - acc: 0.8379\n",
      "Epoch 00002: val_loss improved from 0.39125 to 0.37876, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 0.3567 - acc: 0.8379 - val_loss: 0.3788 - val_acc: 0.8249\n",
      "Epoch 3/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3282 - acc: 0.8550\n",
      "Epoch 00003: val_loss improved from 0.37876 to 0.36871, saving model to my_checkpoint.ckpt\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.3282 - acc: 0.8550 - val_loss: 0.3687 - val_acc: 0.8323\n",
      "Epoch 4/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3156 - acc: 0.8587\n",
      "Epoch 00004: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 0.3156 - acc: 0.8587 - val_loss: 0.3733 - val_acc: 0.8284\n",
      "Epoch 5/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3023 - acc: 0.8702\n",
      "Epoch 00005: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 76s 122ms/step - loss: 0.3023 - acc: 0.8702 - val_loss: 0.4008 - val_acc: 0.8156\n",
      "Epoch 6/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2902 - acc: 0.8743\n",
      "Epoch 00006: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 77s 123ms/step - loss: 0.2902 - acc: 0.8743 - val_loss: 0.3884 - val_acc: 0.8335\n",
      "Epoch 7/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2812 - acc: 0.8802\n",
      "Epoch 00007: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.2812 - acc: 0.8802 - val_loss: 0.3813 - val_acc: 0.8295\n",
      "Epoch 8/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2713 - acc: 0.8839\n",
      "Epoch 00008: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 0.2713 - acc: 0.8839 - val_loss: 0.3892 - val_acc: 0.8253\n",
      "Epoch 9/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2635 - acc: 0.8882\n",
      "Epoch 00009: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.2635 - acc: 0.8882 - val_loss: 0.4246 - val_acc: 0.8214\n",
      "Epoch 10/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2532 - acc: 0.8928\n",
      "Epoch 00010: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 0.2532 - acc: 0.8928 - val_loss: 0.4128 - val_acc: 0.8225\n",
      "Epoch 11/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2482 - acc: 0.8966\n",
      "Epoch 00011: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 0.2482 - acc: 0.8966 - val_loss: 0.4006 - val_acc: 0.8243\n",
      "Epoch 12/12\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2433 - acc: 0.8979\n",
      "Epoch 00012: val_loss did not improve from 0.36871\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 0.2433 - acc: 0.8979 - val_loss: 0.4141 - val_acc: 0.8234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd97f6d590>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data = (X_valid_padded, y_valid),\n",
    "    callbacks=[cp],\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Model Evaludate & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbdb2b4e1d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 7s 32ms/step - loss: 0.3687 - acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36871227622032166, 0.8323147892951965]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid_padded, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/sarcasm_3687.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Reload Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = tf.keras.models.load_model('./model/sarcasm_3687.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           16000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 120, 128)          41472     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 120, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 259,777\n",
      "Trainable params: 259,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('dataScience': conda)",
   "name": "python377jvsc74a57bd088212c6b79be9b544bafe6735d74fe8c206b1cc2a4f62e98b323686df1ed5be2"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}