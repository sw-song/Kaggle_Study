{"cells":[{"cell_type":"markdown","metadata":{},"source":["> 본 커널은 [XGBoost Starter - LB 0.793](https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793)에 작성된 코드를 기반으로 한국어 설명을 추가한 XGBoost 튜토리얼입니다."]},{"cell_type":"markdown","metadata":{},"source":["> TOC\n","```\n","Step 1. Load Libraries\n","Step 2. Load Dataset\n","Step 3. Feature Engineering\n","Step 4. Train XGB\n","Step 5. Save OOF Preds\n","Step 6. Feature Importances\n","Step 7. Process and Feature Engineer Test Data\n","Step 8. Infer Test\n","Step 9. Create Submission CSV\n","```"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Load Libraries"]},{"cell_type":"markdown","metadata":{},"source":["우리가 머신러닝, 딥러닝을 학습시킬 때 CPU를 사용하여 데이터 전처리 등을 수행하고 GPU로 모델 학습을 하는 것이 일반적입니다. 그리고 이 때는 CPU에 올라간 데이터를 GPU로 복사(이동)하는 과정이 필요합니다. pandas로 데이터프레임을 다루고 torch로 GPU 메모리 상의 데이터를 처리하는 식이죠.\n","\n","이제는 그러지 말고, '전체 과정을 모두 GPU 위에서 진행하자' 라는 컨셉으로 나온게 RAPIDS입니다. RAPIDS는 엔비디아가 주도하여 구축하고 운영하고 있는 CUDA 프로세스 기반 데이터사이언스 플랫폼입니다.\n","\n","CUDA를 기반으로 한다는 것을 강조하듯 패키지 이름도 모두 cuxx로 지었습니다. pandas는 cudf로, numpy는 cupy로, sklearn은 cuml로 대체하여 거의 기존과 동일한 함수를 사용할 수 있게 개발되어 있습니다."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:36.984852Z","iopub.status.busy":"2022-07-15T12:48:36.984062Z","iopub.status.idle":"2022-07-15T12:48:38.671830Z","shell.execute_reply":"2022-07-15T12:48:38.670936Z","shell.execute_reply.started":"2022-07-15T12:48:36.984734Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import cupy\n","import cudf\n","\n","import matplotlib.pyplot as plt, gc, os\n","\n","print('cudf version',cudf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Load Dataset\n","\n","cudf를 사용해볼 것입니다. 기본적인 함수나 사용법은 pandas와 동일합니다."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:40.134717Z","iopub.status.busy":"2022-07-15T12:48:40.134065Z","iopub.status.idle":"2022-07-15T12:48:42.516984Z","shell.execute_reply":"2022-07-15T12:48:42.516125Z","shell.execute_reply.started":"2022-07-15T12:48:40.134681Z"},"trusted":true},"outputs":[],"source":["# read_parquet() 함수는 parquet 형식의 파일을 읽어옵니다.\n","df = cudf.read_parquet('../input/amex-data-integer-dtypes-parquet-format/train.parquet')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:42.519119Z","iopub.status.busy":"2022-07-15T12:48:42.518741Z","iopub.status.idle":"2022-07-15T12:48:42.821923Z","shell.execute_reply":"2022-07-15T12:48:42.820556Z","shell.execute_reply.started":"2022-07-15T12:48:42.519082Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["`customer_ID`를 숫자로 깔끔하게 변환해주고 싶다면, hex_to_int() 함수를 사용하고 astype()을 통해 처리할 수 있습니다.\n","여기서는 마지막 16문자에 대해서 해당 처리를 수행합니다."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:42.824404Z","iopub.status.busy":"2022-07-15T12:48:42.823910Z","iopub.status.idle":"2022-07-15T12:48:43.185683Z","shell.execute_reply":"2022-07-15T12:48:43.184634Z","shell.execute_reply.started":"2022-07-15T12:48:42.824357Z"},"trusted":true},"outputs":[],"source":["df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n","df"]},{"cell_type":"markdown","metadata":{},"source":["`S_2`열은 시간 정보를 담고 있습니다. pandas와 마찬가지로 to_datetime()함수로 데이터 타입을 변경해줍니다."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:43.188465Z","iopub.status.busy":"2022-07-15T12:48:43.188048Z","iopub.status.idle":"2022-07-15T12:48:43.484810Z","shell.execute_reply":"2022-07-15T12:48:43.483590Z","shell.execute_reply.started":"2022-07-15T12:48:43.188429Z"},"trusted":true},"outputs":[],"source":["df['S_2'] = cudf.to_datetime(df['S_2'])\n","df"]},{"cell_type":"markdown","metadata":{},"source":["빈 셀도 있으니 fillna() 함수로 채우는데, 1byte(8bit)중 가장 낮은 숫자인 -127을 넣어줄 것입니다."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:43.486950Z","iopub.status.busy":"2022-07-15T12:48:43.486572Z","iopub.status.idle":"2022-07-15T12:48:43.736694Z","shell.execute_reply":"2022-07-15T12:48:43.735537Z","shell.execute_reply.started":"2022-07-15T12:48:43.486913Z"},"trusted":true},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:43.738676Z","iopub.status.busy":"2022-07-15T12:48:43.738275Z","iopub.status.idle":"2022-07-15T12:48:44.155899Z","shell.execute_reply":"2022-07-15T12:48:44.155075Z","shell.execute_reply.started":"2022-07-15T12:48:43.738637Z"},"trusted":true},"outputs":[],"source":["NAN_VALUE = -127 \n","df = df.fillna(NAN_VALUE)\n","df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["아래 함수는 이 과정을 한번에 수행해줍니다."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.157837Z","iopub.status.busy":"2022-07-15T12:48:44.157461Z","iopub.status.idle":"2022-07-15T12:48:44.164277Z","shell.execute_reply":"2022-07-15T12:48:44.163191Z","shell.execute_reply.started":"2022-07-15T12:48:44.157790Z"},"trusted":true},"outputs":[],"source":["def read_file(path = '', usecols = None):\n","    # read_parquet() 함수는 parquet 형식의 파일을 읽어옵니다.\n","    # 이 때, 컬럼을 지정해주고 싶다면\n","    if usecols is not None: \n","        df = cudf.read_parquet(path, columns=usecols)\n","    # columns를 지정하지 않고 그대로 불러온다면,\n","    else: df = cudf.read_parquet(path)\n","    \n","    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n","    df.S_2 = cudf.to_datetime( df.S_2 )\n","    df = df.fillna(NAN_VALUE) \n","    print('shape of data:', df.shape)\n","    \n","    return df\n","\n","# print('Reading train data...')\n","# TRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\n","# train = read_file(path = TRAIN_PATH)"]},{"cell_type":"markdown","metadata":{},"source":["원본 커널에서는 df가 아닌 train 변수명을 사용했습니다. 같은 변수 명을 유지하기 위해 복사해줍니다.\n","\n","단, 여기서 copy()함수를 사용하지 않습니다. 데이터 사이즈가 크기때문에 copy()함수를 사용하면 메모리에 부담을 줄 수 있으니 메모리 참조 방식을 의도적으로 사용합니다. "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.166386Z","iopub.status.busy":"2022-07-15T12:48:44.165703Z","iopub.status.idle":"2022-07-15T12:48:44.390903Z","shell.execute_reply":"2022-07-15T12:48:44.390011Z","shell.execute_reply.started":"2022-07-15T12:48:44.166320Z"},"trusted":true},"outputs":[],"source":["train = df\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Feature Engineering"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-15T10:45:36.492999Z","iopub.status.busy":"2022-07-15T10:45:36.492633Z","iopub.status.idle":"2022-07-15T10:45:36.498767Z","shell.execute_reply":"2022-07-15T10:45:36.497787Z","shell.execute_reply.started":"2022-07-15T10:45:36.492968Z"}},"source":["이번에는 원본 커널에서 사용하는 함수를 그대로 사용할 것입니다. 데이터 집계 및 병합하는 과정에서 multiindex를 1차원으로 묶는 부분만 미리 살펴봅니다."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.394063Z","iopub.status.busy":"2022-07-15T12:48:44.393651Z","iopub.status.idle":"2022-07-15T12:48:44.404968Z","shell.execute_reply":"2022-07-15T12:48:44.403831Z","shell.execute_reply.started":"2022-07-15T12:48:44.394024Z"},"trusted":true},"outputs":[],"source":["train.info()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.407119Z","iopub.status.busy":"2022-07-15T12:48:44.406596Z","iopub.status.idle":"2022-07-15T12:48:44.514779Z","shell.execute_reply":"2022-07-15T12:48:44.513734Z","shell.execute_reply.started":"2022-07-15T12:48:44.407082Z"},"trusted":true},"outputs":[],"source":["multi_index_col_sample = train.groupby('customer_ID')[['B_30','B_38','D_114']].agg(['count','last','nunique']).columns\n","multi_index_col_sample"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.516823Z","iopub.status.busy":"2022-07-15T12:48:44.516401Z","iopub.status.idle":"2022-07-15T12:48:44.524713Z","shell.execute_reply":"2022-07-15T12:48:44.523559Z","shell.execute_reply.started":"2022-07-15T12:48:44.516772Z"},"trusted":true},"outputs":[],"source":["['_'.join(x) for x in multi_index_col_sample]"]},{"cell_type":"markdown","metadata":{},"source":["이렇게 이중 인덱스를 1차원 인덱스로 붙여서 보기 쉽게 만들어줄 수 있습니다. 이어서 전체 함수를 확인합니다."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:44.527230Z","iopub.status.busy":"2022-07-15T12:48:44.526493Z","iopub.status.idle":"2022-07-15T12:48:45.770751Z","shell.execute_reply":"2022-07-15T12:48:45.769902Z","shell.execute_reply.started":"2022-07-15T12:48:44.527189Z"},"trusted":true},"outputs":[],"source":["def process_and_feature_engineer(df):\n","    # list comprehension 방식으로 customer_ID 컬럼과 S_2 컬럼을 제외한 나머지 컬럼명을 all_cols 변수에 담아줍니다. \n","    # 해당 컬럼들은 인덱스와 ID값이 아닌 분석 대상 데이터를 가지고 있습니다.\n","    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n","    \n","    # all_cols 중에서 카테고리형 변수와 수치형 변수를 나눠줍니다.\n","    # 원본 커널에서 카테고리형 변수는 아래 11개 컬럼만 지정하고 있으나, 더 많은 컬럼이 있는 것으로 보입니다.\n","    # 하지만 연속성을 유지하고 혼선을 방지하기 위해 그대로 사용하도록 하겠습니다.\n","    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n","    num_features = [col for col in all_cols if col not in cat_features]\n","\n","    # 각 customer_ID에 대해 수치형 변수들을 통계치로 집계해줍니다.\n","    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n","    # 집계한 컬럼 명은 MultiIndex 타입입니다. '_' 문자로 컬럼명을 이어줍니다.\n","    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n","\n","    # 각 customer_ID에 대해 카테고리형 변수를 집계해줍니다.\n","    # count는 동일한 customer_ID가 몇번 등장하는지, last는 해당 customer_ID의 각 카테고리형 변수 중 가장 최근 값이 무엇인지 보여줍니다.\n","    # nunique()는 각 카테고리형 변수에 등장하는 유일한 값을 세어줍니다.\n","    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n","    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n","\n","    # 기존 데이터와 통계치를 구한 값을 모두 병합해줍니다.\n","    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n","    del test_num_agg, test_cat_agg\n","    print('shape after engineering', df.shape )\n","    \n","    return df\n","\n","\n","train = process_and_feature_engineer(train)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:45.773455Z","iopub.status.busy":"2022-07-15T12:48:45.772013Z","iopub.status.idle":"2022-07-15T12:48:46.666820Z","shell.execute_reply":"2022-07-15T12:48:46.665972Z","shell.execute_reply.started":"2022-07-15T12:48:45.773407Z"},"trusted":true},"outputs":[],"source":["train"]},{"cell_type":"markdown","metadata":{},"source":["데이터셋은 타겟(label) 데이터도 제공하고 있습니다. 위에서 생성한 train 데이터셋에 병합해주겠습니다.\n","이 때, customer_ID는 앞에서 했던 것과 동일한 방식으로 정수형 처리를 해줘야 합니다. 본 테스크는 해당 customer가 대출을 갚을 것인가, 갚지 못할 것인가를 예측하는 과제입니다. customer 마다의 대출 상환 여부가 train_labels.csv에 담겨있습니다."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:46.669673Z","iopub.status.busy":"2022-07-15T12:48:46.669109Z","iopub.status.idle":"2022-07-15T12:48:46.969676Z","shell.execute_reply":"2022-07-15T12:48:46.968860Z","shell.execute_reply.started":"2022-07-15T12:48:46.669634Z"},"trusted":true},"outputs":[],"source":["targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\n","targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n","targets = targets.set_index('customer_ID')\n","\n","# 인덱스는 customer_ID로 동일합니다. 해당 인덱스를 기준으로 merge합니다.\n","train = train.merge(targets, left_index=True, right_index=True, how='left')\n","# 타겟 데이터는 8비트 정수형으로 담아줍니다.\n","train.target = train.target.astype('int8')\n","# targets는 이제 train데이터셋에 병합되었으므로 메모리 상에서 제거해줍니다.\n","del targets"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:46.971503Z","iopub.status.busy":"2022-07-15T12:48:46.971109Z","iopub.status.idle":"2022-07-15T12:48:48.326983Z","shell.execute_reply":"2022-07-15T12:48:48.326094Z","shell.execute_reply.started":"2022-07-15T12:48:46.971469Z"},"trusted":true},"outputs":[],"source":["train = train.reset_index()\n","train"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:15:43.092723Z","iopub.status.busy":"2022-07-15T11:15:43.092352Z","iopub.status.idle":"2022-07-15T11:15:43.098262Z","shell.execute_reply":"2022-07-15T11:15:43.097264Z","shell.execute_reply.started":"2022-07-15T11:15:43.092695Z"}},"source":["모델에 학습시킬 피처 수를 세어봅니다. 첫번째 열은 id값이고 마지막 열은 label입니다. 2개 열을 제외한 나머지 열의 수는 198개입니다."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:48.329078Z","iopub.status.busy":"2022-07-15T12:48:48.328626Z","iopub.status.idle":"2022-07-15T12:48:48.335481Z","shell.execute_reply":"2022-07-15T12:48:48.334228Z","shell.execute_reply.started":"2022-07-15T12:48:48.329035Z"},"trusted":true},"outputs":[],"source":["FEATURES = train.columns[1:-1]\n","print(f'There are {len(FEATURES)} features!')"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4. Train XGB\n","모델을 학습할 때는 KFold 교차검증을 활용해 모든 데이터를 최소 1회 이상 학습에 사용하면서 과적합을 막을 것입니다.\n","일반적으로 편의상 사용되는 70:30 혹은 80:20 등으로 데이터를 학습/검증 용으로 나눠 학습시키는 방식은 30, 20에 해당하는 부분은 학습에 활용할 수 없다는 문제가 있습니다. KFold는 학습데이터와 검증데이터를 쪼개서(K개의 Fold를 만든다고 표현합니다) 모든 데이터셋이 학습에 활용될 수 있도록 합니다."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:48.337730Z","iopub.status.busy":"2022-07-15T12:48:48.337054Z","iopub.status.idle":"2022-07-15T12:48:48.375665Z","shell.execute_reply":"2022-07-15T12:48:48.374192Z","shell.execute_reply.started":"2022-07-15T12:48:48.337686Z"},"trusted":true},"outputs":[],"source":["# LOAD XGB LIBRARY\n","from sklearn.model_selection import KFold\n","import xgboost as xgb\n","print('XGB Version',xgb.__version__)\n","\n","# XGB MODEL PARAMETERS\n","xgb_parms = { \n","    'max_depth':4, \n","    'learning_rate':0.05, \n","    'subsample':0.8,\n","    'colsample_bytree':0.6, \n","    'eval_metric':'logloss',\n","    'objective':'binary:logistic',\n","    'tree_method':'gpu_hist',\n","    'predictor':'gpu_predictor',\n","    'random_state':42\n","}"]},{"cell_type":"markdown","metadata":{},"source":["학습할 때, DeviceQuantileDMatrix를 사용합니다. GPU는 기본적으로 한번 사용시 모든 메모리가 일시에 연산에 사용되는데, 해당 함수를 사용하면 \n","GPU 메모리를 작은 단위로 분할해서 사용하게 해줍니다. \n","\n","DeviceQuantileDMatrix를 사용하기 위해서는 iteration 방식, 즉 next() 함수를 반복 호출하며 배치 형태로 데이터를 넘겨줄 수 있는 클래스를 정의해줘야 합니다."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:48.379202Z","iopub.status.busy":"2022-07-15T12:48:48.378156Z","iopub.status.idle":"2022-07-15T12:48:48.394232Z","shell.execute_reply":"2022-07-15T12:48:48.393236Z","shell.execute_reply.started":"2022-07-15T12:48:48.379126Z"},"trusted":true},"outputs":[],"source":["class IterLoadForDMatrix(xgb.core.DataIter):\n","    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n","        self.features = features\n","        self.target = target\n","        self.df = df\n","        # 0번부터 시작해 데이터를 모두 넘겨줄때까지 1씩 올려줄 것입니다.\n","        self.it = 0 \n","        self.batch_size = batch_size\n","        # np.ceil()은 '올림'을 해주는 함수입니다. \n","        # 데이터를 배치 사이즈로 나눠 배치 크기(갯수)를 계산합니다.\n","        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n","        super().__init__()\n","\n","    def reset(self):\n","        '''Reset the iterator'''\n","        # iteration을 처음부터 다시 실시해줘야 한다면 reset() 함수로 초기화할 수 있습니다.\n","        self.it = 0\n","\n","    def next(self, input_data):\n","        '''Yield next batch of data.'''\n","        # 클래스 인스턴스 생성시 self.batches가 정의되었습니다. 전달할 수 있는 총 배치 수를 담고 있습니다.\n","        # self.it이 전달 가능한 배치 수에 도달했을 때, iteration을 종료합니다.\n","        if self.it == self.batches:\n","            # \n","            return 0 \n","        \n","        # 인덱싱을 위한 start-end point를 구합니다.\n","        a = self.it * self.batch_size\n","        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n","        # 배치로 전달할 데이터를 인덱싱해서 dt 변수에 담아줍니다.\n","        dt = cudf.DataFrame(self.df.iloc[a:b])\n","        # dt로 받은 데이터에서 피처와 타겟을 넘겨줍니다.\n","        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n","        \n","        # iter를 1씩 올려주면서 다음 배치를 수행하기 위한 계산입니다.\n","        self.it += 1\n","        return 1"]},{"cell_type":"markdown","metadata":{},"source":["아래 함수는 본 데이터를 공유한 [American Express - Default Prediction](https://www.kaggle.com/competitions/amex-default-prediction) 대회에서 모델을 평가하는 로직입니다.([원문](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534))\n","\n","모델 학습 과정에서 최적화하는 기준으로 사용하게 됩니다.\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:48.771838Z","iopub.status.busy":"2022-07-15T12:48:48.771477Z","iopub.status.idle":"2022-07-15T12:48:48.782387Z","shell.execute_reply":"2022-07-15T12:48:48.781515Z","shell.execute_reply.started":"2022-07-15T12:48:48.771809Z"},"trusted":true},"outputs":[],"source":["def amex_metric_mod(y_true, y_pred):\n","\n","    labels     = np.transpose(np.array([y_true, y_pred]))\n","    labels     = labels[labels[:, 1].argsort()[::-1]]\n","    weights    = np.where(labels[:,0]==0, 20, 1)\n","    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n","    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n","\n","    gini = [0,0]\n","    for i in [1,0]:\n","        labels         = np.transpose(np.array([y_true, y_pred]))\n","        labels         = labels[labels[:, i].argsort()[::-1]]\n","        weight         = np.where(labels[:,0]==0, 20, 1)\n","        weight_random  = np.cumsum(weight / np.sum(weight))\n","        total_pos      = np.sum(labels[:, 0] *  weight)\n","        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n","        lorentz        = cum_pos_found / total_pos\n","        gini[i]        = np.sum((lorentz - weight_random) * weight)\n","\n","    return 0.5 * (gini[1]/gini[0] + top_four)"]},{"cell_type":"markdown","metadata":{},"source":["드디어 모델을 학습시킵니다. 제한된 GPU를 효율적으로 사용하기 위해 데이터셋을 CPU로 먼저 내려줄 것입니다. 사실, RAPIDS 플랫폼을 사용한다면 모든 과정을 GPU로 수행하는 것이 효과적이지만 아직 온전하게 적용되기에는 어려운 것 같습니다.\n","\n","아무튼, to_pandas() 함수를 사용하면 pandas dataframe 객체로 들어가면서 CPU 자원으로 복사할 수 있습니다."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:50.004478Z","iopub.status.busy":"2022-07-15T12:48:50.004067Z","iopub.status.idle":"2022-07-15T12:48:54.476471Z","shell.execute_reply":"2022-07-15T12:48:54.475552Z","shell.execute_reply.started":"2022-07-15T12:48:50.004447Z"},"trusted":true},"outputs":[],"source":["train = train.to_pandas()"]},{"cell_type":"markdown","metadata":{},"source":["이렇게 했을 때, 여전히 GPU상에서 여러차례 동일한 데이터셋이 저장된 메모리를 참조한 변수들이 있을 것입니다. 그러한 변수들을 깔끔하게 제거해줘야 원활한 GPU 사용이 가능할 것입니다.\n","\n","파이썬은 내부적으로 Garbage Collection이 생성된 객체를 순회하며 '사용되지 않는 객체'를 자동으로 할당 해제 해주는 프로세스를 가지고 있습니다.\n","해당 프로세스가 '사용되지 않는 객체'를 현재 시점에서 인식할 수 있도록 gc.collecter()를 실행해줍니다. 실행하면 메모리 해제된 객체 숫자를 반환해줍니다."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:54.478619Z","iopub.status.busy":"2022-07-15T12:48:54.478210Z","iopub.status.idle":"2022-07-15T12:48:54.616525Z","shell.execute_reply":"2022-07-15T12:48:54.615639Z","shell.execute_reply.started":"2022-07-15T12:48:54.478579Z"},"trusted":true},"outputs":[],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["KFold의 K는 5개로 지정할 것입니다. 그러면 5개의 Fold에 대해 4개의 학습 폴드, 1개의 검증 폴드를 가지게 되고, 검증 폴드의 위치를 옮겨가며 총 5회 반복 학습을 시행합니다. 결과적으로 5번의 검증 결과에 대한 평균값을 통해 최적화를 해나가게 됩니다.\n","\n","SEED는 임의의 숫자로 지정하면 됩니다."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:48:54.618534Z","iopub.status.busy":"2022-07-15T12:48:54.617738Z","iopub.status.idle":"2022-07-15T12:48:54.627785Z","shell.execute_reply":"2022-07-15T12:48:54.626959Z","shell.execute_reply.started":"2022-07-15T12:48:54.618494Z"},"trusted":true},"outputs":[],"source":["FOLDS = 5\n","SEED = 42\n","skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T12:52:15.810187Z","iopub.status.busy":"2022-07-15T12:52:15.809692Z","iopub.status.idle":"2022-07-15T13:01:50.179643Z","shell.execute_reply":"2022-07-15T13:01:50.178589Z","shell.execute_reply.started":"2022-07-15T12:52:15.810122Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["importances = []\n","oof = []\n","TRAIN_SUBSAMPLE = 1.0\n","VER = 1 # 모델을 저장할 때 기록할 버전 정보입니다.\n","\n","# KFold 객체 skf에 대해 교차검증을 수행하기 위하여 학습, 검증 폴드로 분리한 다음 순회하며 반복 검증을 실시합니다.\n","for fold,(train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n","    \n","    # train data 중에서도 일부만 샘플링해서 학습을 수행하고 싶다면 TRAIN_SUBSAMPLE을 1 미만으로 설정해줍니다.\n","    # 그럼 아래 if문을 통해 그 비율만큼 랜덤으로 추출해서 다시 train set을 구성할 수 있습니다.\n","    if TRAIN_SUBSAMPLE<1.0:\n","        np.random.seed(SEED)\n","        train_idx = np.random.choice(train_idx, int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n","        np.random.seed(None)\n","    \n","    print('#'*25)\n","    print('### Fold',fold+1)\n","    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n","    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n","    print('#'*25)\n","    \n","    # 앞에서 만들어둔 IterLoadForDMatrix 객체 인스턴스를 생성해줍니다. 배치로 던져주며 학습하기 위함입니다.\n","    Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, 'target')\n","    \n","    # 학습을 수행하면서 성능을 검증해줄 검증 데이터도 정의해줍니다.\n","    X_valid = train.loc[valid_idx, FEATURES]\n","    y_valid = train.loc[valid_idx, 'target']\n","    \n","    # 이제 DeviceQuantileDMatrix를 사용해 iteration을 돌면서 학습할 수 있는 dtrain 객체를 만들어주고,\n","    dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n","    # 마찬가지로 train 데이터와 같은 형태를 취해 비교연산이 가능하도록 DMatrix로 처리해줍니다.\n","    dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n","    \n","    # 모델을 학습합니다.\n","    model = xgb.train(xgb_parms, \n","                dtrain=dtrain,\n","                evals=[(dtrain,'train'),(dvalid,'valid')],\n","                num_boost_round=9999,\n","                early_stopping_rounds=100,\n","                verbose_eval=100) \n","    # 교차 검증 중 1회 검증시마다 모델을 저장해줍니다.\n","    model.save_model(f'XGB_v{VER}_fold{fold}.xgb')\n","    \n","    # 모델 학습이 끝나고 feature importance를 확인할 것입니다. 이를 위해 매 학습마다 importance를 계산하여 변수에 저장해줍니다.\n","    dd = model.get_score(importance_type='weight')\n","    df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n","    importances.append(df)\n","            \n","    # 모델을 검증합니다. 정확도는 위에서 함수로 만들어둔 대회 평가지표를 사용합니다.\n","    oof_preds = model.predict(dvalid)\n","    acc = amex_metric_mod(y_valid.values, oof_preds)\n","    print('Kaggle Metric =',acc,'\\n')\n","    \n","    # 검증 스코어(oof_pred)도 따로 저장해줍니다.\n","    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n","    df['oof_pred'] = oof_preds\n","    oof.append( df )\n","    \n","    # 학습을 위해 사용한 변수들은 모두 메모리 해제해줍니다.\n","    del dtrain, Xy_train, dd, df\n","    del X_valid, y_valid, dvalid, model\n","    # 메모리 참조된 것들도 완전히 제거하기 위해 가비지 컬랙션을 실시해줍니다.\n","    _ = gc.collect()\n","    \n","print('#'*25)\n","# 학습이 종료되면 전체 검증 결과를 데이터프레임으로 저장해주고, \n","# 실제 값과 검증 값을 평가지표로 계산해줍니다.\n","oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n","acc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n","print('OVERALL CV Kaggle Metric =',acc)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:03:42.714212Z","iopub.status.busy":"2022-07-15T13:03:42.713785Z","iopub.status.idle":"2022-07-15T13:03:42.893831Z","shell.execute_reply":"2022-07-15T13:03:42.891797Z","shell.execute_reply.started":"2022-07-15T13:03:42.714170Z"},"trusted":true},"outputs":[],"source":["# 학습이 종료되었으니 이제 데이터셋도 필요가 없어졌습니다.\n","# 메모리를 한번 더 정리해주겠습니다.\n","del train\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5. Save OOF Preds"]},{"cell_type":"markdown","metadata":{},"source":["예측 결과를 customer_ID와 함께 저장해줄 것입니다. 데이터 파일에서 unique한 id 정보만 가져와 앞에서 수행했던 것처럼 16진수를 정수형으로 바꿔주고, 학습 과정에서 확보한 예측값을 병합해주면 됩니다."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:04:05.934154Z","iopub.status.busy":"2022-07-15T13:04:05.933721Z","iopub.status.idle":"2022-07-15T13:04:11.372153Z","shell.execute_reply":"2022-07-15T13:04:11.371251Z","shell.execute_reply.started":"2022-07-15T13:04:05.934104Z"},"trusted":true},"outputs":[],"source":["TRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\n","oof_xgb = pd.read_parquet(TRAIN_PATH, columns=['customer_ID']).drop_duplicates()\n","oof_xgb['customer_ID_hash'] = oof_xgb['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n","oof_xgb = oof_xgb.set_index('customer_ID_hash')\n","oof_xgb = oof_xgb.merge(oof, left_index=True, right_index=True)\n","oof_xgb = oof_xgb.sort_index().reset_index(drop=True)\n","oof_xgb.to_csv(f'oof_xgb_v{VER}.csv',index=False)\n","oof_xgb.head()"]},{"cell_type":"markdown","metadata":{},"source":["예측 결과를 시각화해봅니다. 예측 결과는 0에서 1사이의 확률입니다. 단 5%만 채무 불이행(default, 1)이고 나머지는 0으로 예측해야 하기 때문에 시각화했을 때 0과 1쪽에 양방향으로 쏠려있되, 0에 더 많은 값이 분포되어 있어야 합니다."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:04:15.498961Z","iopub.status.busy":"2022-07-15T13:04:15.498583Z","iopub.status.idle":"2022-07-15T13:04:15.897233Z","shell.execute_reply":"2022-07-15T13:04:15.896252Z","shell.execute_reply.started":"2022-07-15T13:04:15.498930Z"},"trusted":true},"outputs":[],"source":["plt.hist(oof_xgb.oof_pred.values, bins=100)\n","plt.title('OOF Predictions')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["예측치를 csv 파일로 저장했으니 변수를 제거합니다. 이렇게 파일로 저장하고 메모리를 비우는 작업을 반복하는 이유는 캐글 상에서 지원되는 RAM이 크지 않기 때문입니다. 일반적으로 로컬 환경에서도 RAM은 여유롭지 않기 때문에 중간에 참조나 복사를 하면서 셧다운 되는 일이 발생할 수 있는데, 이를 방지해주기 위해 하드디스크로 파일을 저장하고 RAM은 가벼운 상태를 유지해주는 것이 좋습니다."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:04:18.758601Z","iopub.status.busy":"2022-07-15T13:04:18.757752Z","iopub.status.idle":"2022-07-15T13:04:18.934057Z","shell.execute_reply":"2022-07-15T13:04:18.933008Z","shell.execute_reply.started":"2022-07-15T13:04:18.758556Z"},"trusted":true},"outputs":[],"source":["del oof_xgb, oof\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 6. Feature Importance"]},{"cell_type":"markdown","metadata":{},"source":["Feature Importance는 피처 중요도, 즉 모델이 테스크를 수행(여기서는 예측)하는데에 어떤 변수가 활용 비중이 높은가에 대한 정보입니다.\n","모델 학습이 끝난 다음 이렇게 살펴보면서 예측에 중요하지 않은 변수가 있다면 제거하고 중요도가 지나치게 변수들은 예측 대상과의 인과성 혹은 상관성을 확인하는 등의 피드백 과정을 거치게 됩니다."]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:12:33.654914Z","iopub.status.busy":"2022-07-15T13:12:33.654393Z","iopub.status.idle":"2022-07-15T13:12:33.693374Z","shell.execute_reply":"2022-07-15T13:12:33.692397Z","shell.execute_reply.started":"2022-07-15T13:12:33.654874Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# 교차검증을 수행하면서 FOLD 수(5)만큼 importances도 누적해서 구해졌을 것입니다.\n","# df 변수에 병합하여 feature마다 평균 importance를 계산해줍니다.\n","df = importances[0].copy()\n","for k in range(1,FOLDS):\n","    df = df.merge(importances[k], on='feature', how='left')\n","df['importance'] = df.iloc[:,1:].mean(axis=1)\n","df = df.sort_values('importance',ascending=False)\n","df.to_csv(f'xgb_feature_importance_v{VER}.csv',index=False)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:12:37.324126Z","iopub.status.busy":"2022-07-15T13:12:37.323528Z","iopub.status.idle":"2022-07-15T13:12:37.363282Z","shell.execute_reply":"2022-07-15T13:12:37.362278Z","shell.execute_reply.started":"2022-07-15T13:12:37.324078Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["bar 차트로 중요도 기준 상위 20개 feature만 시각화해줍니다."]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:13:21.110322Z","iopub.status.busy":"2022-07-15T13:13:21.109885Z","iopub.status.idle":"2022-07-15T13:13:21.412063Z","shell.execute_reply":"2022-07-15T13:13:21.411201Z","shell.execute_reply.started":"2022-07-15T13:13:21.110288Z"},"trusted":true},"outputs":[],"source":["NUM_FEATURES = 20\n","plt.figure(figsize=(10,5*NUM_FEATURES//10))\n","plt.barh(np.arange(NUM_FEATURES,0,-1), df.importance.values[:NUM_FEATURES])\n","plt.yticks(np.arange(NUM_FEATURES,0,-1), df.feature.values[:NUM_FEATURES])\n","plt.title(f'XGB Feature Importance - Top {NUM_FEATURES}')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 7. Process and Feature Engineer Test Data"]},{"cell_type":"markdown","metadata":{},"source":["이제 위에서 학습한 xgboost 모델로 테스트 데이터를 예측합니다. 먼저 테스트 데이터를 살펴보겠습니다."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:24:03.251369Z","iopub.status.busy":"2022-07-15T13:24:03.250900Z","iopub.status.idle":"2022-07-15T13:24:05.100855Z","shell.execute_reply":"2022-07-15T13:24:05.099859Z","shell.execute_reply.started":"2022-07-15T13:24:03.251324Z"},"trusted":true},"outputs":[],"source":["TEST_PATH = '../input/amex-data-integer-dtypes-parquet-format/test.parquet'\n","test = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\n","test"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:24:24.685900Z","iopub.status.busy":"2022-07-15T13:24:24.685403Z","iopub.status.idle":"2022-07-15T13:24:24.694192Z","shell.execute_reply":"2022-07-15T13:24:24.692762Z","shell.execute_reply.started":"2022-07-15T13:24:24.685857Z"}},"source":["테스트 데이터 역시 사이즈가 크기 때문에 램에 한번에 올리는 것은 무리가 있겠습니다. 데이터셋을 분할해서 넣어줄 수 있도록 처리해주기 위해 함수를 만들어 주겠습니다."]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:33:25.943893Z","iopub.status.busy":"2022-07-15T13:33:25.943464Z","iopub.status.idle":"2022-07-15T13:33:25.951702Z","shell.execute_reply":"2022-07-15T13:33:25.950720Z","shell.execute_reply.started":"2022-07-15T13:33:25.943859Z"},"trusted":true},"outputs":[],"source":["# 4개 PART로 분리해주는 함수입니다.\n","def get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n","    # 한 청크(1개 PART의 사이즈)는 전체 평가 데이터셋 크기를 PART 수로 나눠주면 구할 수 있습니다.\n","    # 모델 학습할 때 배치 사이즈 구했던 것과 비슷합니다.\n","    chunk = len(customers)//NUM_PARTS\n","    if verbose != '':\n","        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n","        print(f'There will be {chunk} customers in each part (except the last part).')\n","        print('Below are number of rows in each part:')\n","    rows = []\n","\n","    for k in range(NUM_PARTS):\n","        # 마지막 PART면 남은 부분을 모두 cc에 담습니다.\n","        if k==NUM_PARTS-1: \n","            cc = customers[k*chunk:]\n","        # 마지막 PART가 아니면 앞에서부터 청크 단위로 잘라서 cc에 담습니다.\n","        else: \n","            cc = customers[k*chunk:(k+1)*chunk]\n","        # 현재 PART에 포함된 customer_ID 수를 구하는 것을 통해 PART 사이즈를 계산합니다.\n","        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n","        # rows에는 4개 PART의 크기가 담겨있습니다.\n","        rows.append(s)\n","    if verbose != '': print( rows )\n","    return rows, chunk\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["우리는 특정 customer가 지출액을 상환할 것인가가 궁금합니다. 테스트 데이터셋에는 customer_ID가 중복되어 들어가있기 때문에 중복은 제거하고 유니크한 customer_ID만 남겨둡니다. 그리고 데이터프레임 형식을 유지할 필요는 없기 때문에 1차원 데이터로 펼쳐주겠습니다."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:28:19.642229Z","iopub.status.busy":"2022-07-15T13:28:19.641779Z","iopub.status.idle":"2022-07-15T13:28:20.317701Z","shell.execute_reply":"2022-07-15T13:28:20.316832Z","shell.execute_reply.started":"2022-07-15T13:28:19.642195Z"},"trusted":true},"outputs":[],"source":["customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n","customers"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:29:13.319608Z","iopub.status.busy":"2022-07-15T13:29:13.319054Z","iopub.status.idle":"2022-07-15T13:29:13.327749Z","shell.execute_reply":"2022-07-15T13:29:13.326253Z","shell.execute_reply.started":"2022-07-15T13:29:13.319567Z"}},"source":["이제 위에서 만든 함수로 customer_ID를 총 4개 그룹(PART)로 나누겠습니다."]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:29:26.259230Z","iopub.status.busy":"2022-07-15T13:29:26.258784Z","iopub.status.idle":"2022-07-15T13:29:26.692436Z","shell.execute_reply":"2022-07-15T13:29:26.691302Z","shell.execute_reply.started":"2022-07-15T13:29:26.259193Z"},"trusted":true},"outputs":[],"source":["# 테스트 데이터셋을 4개로 나눠주겠습니다.\n","NUM_PARTS = 4\n","rows,num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS = NUM_PARTS, verbose = 'test')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:34:33.019863Z","iopub.status.busy":"2022-07-15T13:34:33.019463Z","iopub.status.idle":"2022-07-15T13:34:33.026719Z","shell.execute_reply":"2022-07-15T13:34:33.025412Z","shell.execute_reply.started":"2022-07-15T13:34:33.019831Z"}},"source":["마지막 줄에 각 PART별 크기가 출력되었습니다. 모두 더했을 때 test 데이터셋의 사이즈와 같아야 합니다."]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:33:40.463045Z","iopub.status.busy":"2022-07-15T13:33:40.462645Z","iopub.status.idle":"2022-07-15T13:33:40.468923Z","shell.execute_reply":"2022-07-15T13:33:40.467875Z","shell.execute_reply.started":"2022-07-15T13:33:40.463017Z"},"trusted":true},"outputs":[],"source":["sum(rows) == len(test)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T13:31:25.219678Z","iopub.status.busy":"2022-07-15T13:31:25.218964Z","iopub.status.idle":"2022-07-15T13:31:25.227567Z","shell.execute_reply":"2022-07-15T13:31:25.226344Z","shell.execute_reply.started":"2022-07-15T13:31:25.219640Z"},"trusted":true},"outputs":[],"source":["customers[0]"]},{"cell_type":"markdown","metadata":{},"source":["## Step 8. Infer Test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# INFER TEST DATA IN PARTS\n","skip_rows = 0\n","skip_cust = 0\n","test_preds = []\n","\n","for k in range(NUM_PARTS):\n","    \n","    # READ PART OF TEST DATA\n","    print(f'\\nReading test data...')\n","    test = read_file(path = TEST_PATH)\n","    test = test.iloc[skip_rows:skip_rows+rows[k]]\n","    skip_rows += rows[k]\n","    print(f'=> Test part {k+1} has shape', test.shape )\n","    \n","    # PROCESS AND FEATURE ENGINEER PART OF TEST DATA\n","    test = process_and_feature_engineer(test)\n","    if k==NUM_PARTS-1: test = test.loc[customers[skip_cust:]]\n","    else: test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n","    skip_cust += num_cust\n","    \n","    # TEST DATA FOR XGB\n","    X_test = test[FEATURES]\n","    dtest = xgb.DMatrix(data=X_test)\n","    test = test[['P_2_mean']] # reduce memory\n","    del X_test\n","    gc.collect()\n","\n","    # INFER XGB MODELS ON TEST DATA\n","    model = xgb.Booster()\n","    model.load_model(f'XGB_v{VER}_fold0.xgb')\n","    preds = model.predict(dtest)\n","    for f in range(1,FOLDS):\n","        model.load_model(f'XGB_v{VER}_fold{f}.xgb')\n","        preds += model.predict(dtest)\n","    preds /= FOLDS\n","    test_preds.append(preds)\n","\n","    # CLEAN MEMORY\n","    del dtest, model\n","    _ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 9. Create Submission CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# WRITE SUBMISSION FILE\n","test_preds = np.concatenate(test_preds)\n","test = cudf.DataFrame(index=customers,data={'prediction':test_preds})\n","sub = cudf.read_csv('../input/amex-default-prediction/sample_submission.csv')[['customer_ID']]\n","sub['customer_ID_hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n","sub = sub.set_index('customer_ID_hash')\n","sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n","sub = sub.reset_index(drop=True)\n","\n","# DISPLAY PREDICTIONS\n","sub.to_csv(f'submission_xgb_v{VER}.csv',index=False)\n","print('Submission file shape is', sub.shape )\n","sub.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# PLOT PREDICTIONS\n","plt.hist(sub.to_pandas().prediction, bins=100)\n","plt.title('Test Predictions')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 ('dataScience')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"vscode":{"interpreter":{"hash":"946e70462398da2b7d5146f365841ff0dae0fe6e2f7c4e5ed0a8dd78c4677798"}}},"nbformat":4,"nbformat_minor":4}
